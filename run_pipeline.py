import os
import json
from glob import glob
from pathlib import Path
from model_run import run_models, analyze_fatigue
from LSTM_pre import LSTMClassifier
import torch.nn.functional as F
import os
import numpy as np
import torch
from facenet_pytorch import MTCNN, InceptionResnetV1
import mediapipe as mp
import os
import time
import cv2
from collections import deque
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import tempfile
import shutil
from collections import deque
from datetime import datetime
#  è·¯å¾„é…ç½®è®°å¾—æ”¹
UPLOAD_DIR = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\uploads"
RESULT_PATH = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\result.json"
FEATURES_PATH = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\features.npy"
EAR_MAR_PATH = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\avg_ear_mar.csv"
LSTM_MODEL_PATH = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\best_lstm_v3.pth"
VIOLATION_LOG_PATH = r"D:\python\æ¼”ç¤ºç”¨\.venv\åç«¯\violation_log.txt"
all_features = deque(maxlen=100)
first_write = True
# ç”¨äºè®°å½•5ç§’å†…çš„çœ¨çœ¼å’Œæ‰“å“ˆæ¬ æ¬¡æ•°
# ç”¨äºè®°å½•è¿‡å»5ç§’å†…çš„ç–²åŠ³è¡Œä¸ºå‘ç”Ÿæ—¶é—´ï¼ˆå•ä½ï¼šç§’æ—¶é—´æˆ³ï¼‰
blink_window = deque()
yawn_window = deque()

def record_violation(msg, violations_list=None, image_name="unknown.jpg"):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    formatted = f"[{timestamp}] Action detectionï¼š{msg}"
    print(formatted)
    with open(VIOLATION_LOG_PATH, "a", encoding="utf-8") as f:
        f.write(formatted + "\n")
    if violations_list is not None:
        violations_list.append(("Action Detection", msg))

# ç›‘æ§å¤„ç†å™¨
class SingleImageWatcher(FileSystemEventHandler):
    def __init__(self):
        self.last_processed_file = None
        self.new_result_count = 0  # è®¡æ•°å™¨

    def try_process(self, path):
        for _ in range(10):  # æœ€å¤šå°è¯•10æ¬¡è¯»å–
            try:
                start=time.time()
                violations = []
                result_data = run_models(path, FEATURES_PATH, EAR_MAR_PATH)
                feats = result_data["features"]
                avg_ear = result_data["avg_ear"]
                avg_mar = result_data["mar"]
                # ============ æ¨¡å‹3ï¼ˆç–²åŠ³æ£€æµ‹ï¼‰ ============
                try:
                    fatigue_result = analyze_fatigue(path, FEATURES_PATH, EAR_MAR_PATH, show_image=False)
                    is_drowsy = fatigue_result.get("is_drowsy", False)
                    reasons = fatigue_result.get("reasons", [])

                    # åˆ¤æ–­æ˜¯å¦é—­çœ¼æˆ–æ‰“å“ˆæ¬ 
                    # === è®°å½•æœ€è¿‘5ç§’å†…çš„ç–²åŠ³è¡Œä¸º ===
                    now = time.time()

                    # ä¿ç•™æœ€è¿‘5ç§’å†…çš„äº‹ä»¶
                    # ä¿ç•™æœ€è¿‘5ç§’å†…çš„äº‹ä»¶ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
                    blink_window_data = [t for t in blink_window if now - t <= 5]
                    yawn_window_data = [t for t in yawn_window if now - t <= 5]
                    blink_window.clear()
                    blink_window.extend(blink_window_data)
                    yawn_window.clear()
                    yawn_window.extend(yawn_window_data)

                    # ç»Ÿè®¡5ç§’å†…ç–²åŠ³è¡Œä¸ºæ¬¡æ•°
                    blink_count = len(blink_window)
                    yawn_count = len(yawn_window)

                except Exception as e:
                    print("âš ï¸ ç–²åŠ³åˆ†æå¤±è´¥", e)
                    blink_count = yawn_count = 0

                all_features.append(feats)
                print(f"âœ… æˆåŠŸæå–: {os.path.basename(path)}ï¼ˆå½“å‰æ£€æµ‹åˆ°é—­çœ¼é˜Ÿåˆ—é•¿åº¦: {len(all_features)}ï¼‰")
                print(feats)
                print(f"{time.time()-start}s")

                self.last_processed_file = path
                self.new_result_count+=1
                if self.new_result_count >= 5 and len(all_features) >= 5:


                    sequence_tensor = torch.tensor(all_features, dtype=torch.float32).unsqueeze(0).to(
                        device)  # shape [1, 60, 12]
                    with torch.no_grad():
                        start2=time.time()
                        output = lstm_model(sequence_tensor)
                        probs = F.softmax(output, dim=1).cpu().numpy()[0]
                        predicted_class = int(np.argmax(probs))
                        print("lstm result is:",predicted_class,time.time()-start2)
                    self.new_result_count = 0  # é‡ç½®è®¡æ•°å™¨
                    # å…¶ä»–è¿è§„è¡Œä¸ºåˆ¤æ–­
                    image_name = os.path.basename(path)

                    # ç–²åŠ³è¡Œä¸º
                    if "é—­çœ¼" in reasons:
                        blink_window.append(now)
                        record_violation("Detected closing of eyes", violations, image_name)

                    if "æ‰“å“ˆæ¬ " in reasons:
                        yawn_window.append(now)
                        record_violation("Detected yawning", violations, image_name)

                    # è¡Œä¸ºæ£€æµ‹
                    if feats[0] > 0.2:
                        record_violation("Detected an object", violations, image_name)

                    if feats[1] < 0.5 and feats[2] > 0.5:
                        record_violation("Detected that the seat belt was not fastened.", violations, image_name)

                    if feats[3] > 0.4:
                        record_violation("Detected an extra hand", violations, image_name)

                    if feats[4] > 0.5 and feats[5] < 0.5:
                        record_violation("Detected that the hand has left the steering wheel", violations, image_name)

                    # æ„é€  JSON è¾“å‡ºç»“æœ
                    CLASS_NAMES = ["no_belt", "disturb", "violating object", "violating object", "normal"]
                    result = {
                        "predicted_class": predicted_class,
                        "class_name": CLASS_NAMES[predicted_class],
                        "class_probabilities": [round(float(p), 4) for p in probs],
                        "objects":int(feats[0]>0.3),
                        "with_seatbelt":int(feats[1]>0.3),
                        "without_seatbelt":int(feats[2]>0.5),
                        "extra_hand":int(feats[3]>0.1),
                        "hands_off":int(feats[4]>0.03),
                        "hands_on":int(feats[5]>0.1),
                        "wheel":feats[6],
                        "name": result_data.get("name", "Unknown"),
                        "avg_ear":avg_ear,
                        "avg_mar":avg_mar,
                        "blink_count": blink_count,
                        "yawn_count": yawn_count,
                        "violations": violations

                    }
                    with tempfile.NamedTemporaryFile('w', delete=False, encoding='utf-8',
                                                     dir=os.path.dirname(RESULT_PATH)) as tmpf:
                        json.dump(result, tmpf, ensure_ascii=False, indent=2)
                        temp_name = tmpf.name
                    shutil.move(temp_name, RESULT_PATH)
                    # å°†æ‰€æœ‰è¿è§„ä¿¡æ¯å†™å…¥æ—¥å¿—
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    with open(VIOLATION_LOG_PATH, "a", encoding="utf-8") as f:
                        for category, message in violations:
                            f.write(f"[{timestamp}] {category}ï¼š{message}\n")

                return
            except Exception as e:
                time.sleep(0.2)
        print(f"âŒ å›¾åƒè¯»å–å¤±è´¥: {os.path.basename(path)}")

    def on_created(self, event):
        if not event.is_directory and event.src_path.endswith(".jpg"):
            self.try_process(event.src_path)

    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith(".jpg"):
            if event.src_path != self.last_processed_file:
                self.try_process(event.src_path)

    def on_deleted(self, event):
        if not event.is_directory and event.src_path == self.last_processed_file:
            print(f"ğŸ—‘ï¸ æ–‡ä»¶è¢«åˆ é™¤: {os.path.basename(event.src_path)}")
            self.last_processed_file = None

# å¯åŠ¨ç›‘æ§
def start_watching(folder):
    observer = Observer()
    handler = SingleImageWatcher()
    observer.schedule(handler, folder, recursive=False)
    observer.start()
    print(f"ğŸ‘ï¸ å®æ—¶ç›‘æ§æ–‡ä»¶å¤¹: {folder}")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()


if __name__ == "__main__":
    # åˆ†ç±»æ ‡ç­¾
    CLASS_NAMES = ["no_belt", "disturb", "phone", "eat/drink", "normal"]
    # åˆå§‹åŒ–
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(os.path.dirname(RESULT_PATH), exist_ok=True)

    #  åŠ è½½ LSTM
    lstm_model = LSTMClassifier(input_size=10, hidden_size=32, output_size=5).to(device)
    lstm_model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))
    lstm_model.eval()

    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

    # ======= æ¨¡å‹åŠ è½½ =======
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    mtcnn = MTCNN(image_size=160, margin=0, keep_all=False, device=device)
    facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)
    face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1,
                                                refine_landmarks=True, min_detection_confidence=0.5)
    start_watching(UPLOAD_DIR)

'''
# åŠ è½½100å¼ å›¾åƒ
image_paths = sorted(glob(os.path.join(UPLOAD_DIR, "*.jpg")))[:60]
if len(image_paths) < 60:
    raise ValueError(" ä¸Šä¼ å›¾åƒä¸è¶³60å¼ ï¼Œå½“å‰ä»…æœ‰ {} å¼ ".format(len(image_paths)))

#  æå–100å¼ å›¾çš„12ç»´ç‰¹å¾
all_features = []
for img_path in image_paths:
    feats = run_models(img_path, FEATURES_PATH, EAR_MAR_PATH)  # è¿”å›12ç»´
    all_features.append(feats)

print(len(all_features))

#  LSTM åˆ†ç±»é¢„æµ‹
sequence_tensor = torch.tensor(all_features, dtype=torch.float32).unsqueeze(0).to(device)  # shape [1, 60, 12]
with torch.no_grad():
    output = lstm_model(sequence_tensor)
    probs = F.softmax(output, dim=1).cpu().numpy()[0]
    predicted_class = int(np.argmax(probs))

print(all_features[len(all_features)-1])
'''
'''
# æ„é€  JSON è¾“å‡ºç»“æœ
result = {
    "predicted_class": predicted_class,
    "class_name": CLASS_NAMES[predicted_class],
    "class_probabilities": [round(float(p), 4) for p in probs],
    "name": fatigue_result.get("name", "Unknown"),
    "similarity": round(float(fatigue_result.get("similarity", 0)), 4),
    "avg_ear": round(float(fatigue_result.get("avg_ear", 0)), 4),
    "mar": round(float(fatigue_result.get("mar", 0)), 4),
    "ear_thresh": round(float(fatigue_result.get("ear_thresh", 0)), 4),
    "mar_thresh": round(float(fatigue_result.get("mar_thresh", 0)), 4),
    "is_drowsy": fatigue_result.get("is_drowsy", False),
    "reasons": fatigue_result.get("reasons", [])
}

with open(RESULT_PATH, "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

# è¾“å‡ºç»“æœ
print("\n[æ¨ç†å®Œæˆï¼šç»Ÿä¸€å¤„ç†60å¼ å›¾åƒ]")
print(f" è¯†åˆ«ç±»åˆ«ï¼š{CLASS_NAMES[predicted_class]}")
print(f" ç–²åŠ³çŠ¶æ€ï¼š{'ç–²åŠ³' if result['is_drowsy'] else 'æ­£å¸¸'}ï¼ˆç†ç”±ï¼š{result['reasons']}ï¼‰")
print(f" ç”¨æˆ·èº«ä»½ï¼š{result['name']}ï¼ˆç›¸ä¼¼åº¦ï¼š{result['similarity']}ï¼‰")
print(f" JSON å·²ä¿å­˜è‡³ï¼š{RESULT_PATH}")
'''